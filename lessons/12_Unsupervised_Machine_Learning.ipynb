{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BME3053C - Computer Applications for BME\n",
        "\n",
        "<br/>\n",
        "\n",
        "<h1 align=\"center\">Unsupervised Machine Learning</h1>\n",
        "\n",
        "---\n",
        "\n",
        "<center><h2>Lesson: 12</h2></center>\n",
        "\n",
        "### Original Lesson Link: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/uf-bme/bme3053c/blob/main/lessons/12_Unsupervised_Machine_Learning.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introduction\n",
        "\n",
        "Unsupervised machine learning refers to a type of machine learning where the model learns from unlabeled data. Unlike supervised learning, there are no target labels for the model to predict. Instead, the model identifies patterns and structures within the data.\n",
        "\n",
        "The goal of unsupervised learning is to find hidden patterns or intrinsic structures in the data. Since the data is unlabeled, the model must determine relationships and groupings without any guidance. Unsupervised learning is particularly useful when we do not know what we are looking for in the data or when we want to explore the data to find inherent groupings or reduce complexity.\n",
        "\n",
        "There are two main types of unsupervised learning:\n",
        "\n",
        "1. **Clustering**: Grouping data points based on similarity. Clustering algorithms attempt to find clusters or groups of similar data points in the dataset. This can be useful for customer segmentation, market research, and understanding natural groupings within data. Examples of clustering algorithms include k-means, hierarchical clustering, and DBSCAN.\n",
        "\n",
        "2. **Dimensionality Reduction**: Reducing the number of features in the dataset while retaining important information. This is particularly useful when dealing with high-dimensional data that can be difficult to visualize or analyze. Dimensionality reduction techniques help in compressing data, removing noise, and making the analysis more efficient. Examples of dimensionality reduction techniques include Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), Independent Component Analysis (ICA), and Factor Analysis.\n",
        "\n",
        "Common applications of unsupervised learning include clustering, association, and dimensionality reduction. In this lesson, we will explore five popular unsupervised learning techniques: **k-means clustering**, **hierarchical clustering**, **DBSCAN**, **principal component analysis (PCA)**, and **t-distributed stochastic neighbor embedding (t-SNE)**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### When to Use Clustering vs Dimensionality Reduction\n",
        "\n",
        "**Clustering Use Cases:**\n",
        "\n",
        "- Customer segmentation: Group customers based on purchasing behavior, demographics, etc.\n",
        "- Disease subtyping: Identify distinct patient groups with similar symptoms/biomarkers\n",
        "- Image segmentation: Group similar pixels to identify objects/regions\n",
        "- Document categorization: Group similar documents by topic/content\n",
        "- Network analysis: Find communities in social networks\n",
        "\n",
        "**Dimensionality Reduction Use Cases:**\n",
        "\n",
        "- Visualizing high-dimensional data: Reduce many features to 2-3 dimensions for plotting\n",
        "- Feature extraction: Combine correlated features into meaningful components\n",
        "- Noise reduction: Remove less important features while preserving key patterns\n",
        "- Data compression: Reduce storage/computation needs while keeping important information\n",
        "- Preprocessing for other algorithms: Improve performance by reducing input dimensions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pros and Cons of Unsupervised Learning Methods\n",
        "\n",
        "**Dimensionality Reduction**\n",
        "\n",
        "_Pros:_\n",
        "\n",
        "- Reduces computational complexity and storage requirements\n",
        "- Helps avoid overfitting by removing noise and redundant features\n",
        "- Makes visualization possible for high-dimensional data\n",
        "- Can improve model performance by removing collinear features\n",
        "- Reduces training time for machine learning models\n",
        "\n",
        "_Cons:_\n",
        "\n",
        "- May lose some information in the reduction process\n",
        "- Reduced features can be harder to interpret\n",
        "- Finding the optimal number of dimensions can be challenging\n",
        "- Some methods (like t-SNE) can be computationally expensive\n",
        "- Risk of removing important features if not done carefully\n",
        "\n",
        "**Clustering**\n",
        "\n",
        "_Pros:_\n",
        "\n",
        "- Discovers natural groupings in data without labels\n",
        "- Useful for customer segmentation and pattern discovery\n",
        "- Can handle various types of data structures\n",
        "- Provides insights into data organization\n",
        "- Helps identify outliers and anomalies\n",
        "\n",
        "_Cons:_\n",
        "\n",
        "- Difficult to validate results without ground truth\n",
        "- Sensitive to initial conditions and parameters\n",
        "- May find clusters that aren't meaningful\n",
        "- Different algorithms can give different results\n",
        "- Challenging to determine optimal number of clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import seaborn as sns\n",
        "except ImportError:\n",
        "    %pip install seaborn\n",
        "    import seaborn as sns\n",
        "\n",
        "try:\n",
        "    from sklearn.cluster import KMeans, DBSCAN\n",
        "    from sklearn.decomposition import PCA\n",
        "    from sklearn.manifold import TSNE\n",
        "    from sklearn.datasets import fetch_openml\n",
        "except ImportError:\n",
        "    %pip install scikit-learn\n",
        "    from sklearn.cluster import KMeans, DBSCAN\n",
        "    from sklearn.decomposition import PCA\n",
        "    from sklearn.manifold import TSNE\n",
        "    from sklearn.datasets import fetch_openml\n",
        "\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Dataset\n",
        "\n",
        "For this lesson, we will use the **Diabetes** dataset, a biomedical dataset that contains information about various features related to diabetes diagnosis. The dataset includes the following features:\n",
        "\n",
        "- preg: Number of pregnancies\n",
        "- plas: Plasma glucose concentration\n",
        "- pres: Diastolic blood pressure (mm Hg)\n",
        "- skin: Triceps skin fold thickness (mm)\n",
        "- insu: 2-Hour serum insulin (mu U/ml)\n",
        "- mass: Body mass index (weight in kg/(height in m)^2)\n",
        "- pedi: Diabetes pedigree function\n",
        "- age: Age (years)\n",
        "- class: Diabetes diagnosis (0 = tested negative, 1 = tested positive)\n",
        "  This dataset has multiple numerical features, making it suitable for dimensionality reduction techniques like t-SNE, which can capture the non-linear relationships between features and reveal hidden structures more effectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Diabetes dataset\n",
        "data = fetch_openml(data_id=37, as_frame=True)  # Load the dataset from OpenML\n",
        "df = data.frame\n",
        "\n",
        "# Convert class column to numeric (0 for negative, 1 for positive)\n",
        "df['class'] = df['class'].map({'tested_negative': 0, 'tested_positive': 1})\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exploratory Data Analysis (EDA)\n",
        "\n",
        "Let's perform some exploratory data analysis to understand the structure of the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pairplot to visualize relationships between a subset of features, colored by class\n",
        "sns.pairplot(df.iloc[:, :], hue='class', palette='Set2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dimensionality Reduction\n",
        "\n",
        "Dimensionality reduction is a crucial technique in machine learning and data analysis that transforms high-dimensional data into a lower-dimensional representation while preserving important information. This process helps to:\n",
        "\n",
        "1. **Visualization**: Convert high-dimensional data into 2D or 3D representations that humans can easily visualize and interpret\n",
        "2. **Computational Efficiency**: Reduce computational complexity and memory requirements by working with fewer dimensions\n",
        "3. **Noise Reduction**: Remove redundant features and noise that might not contribute meaningful information\n",
        "4. **Feature Selection**: Identify the most important features or combinations of features\n",
        "\n",
        "There are two main categories of dimensionality reduction techniques:\n",
        "\n",
        "**Linear Methods**:\n",
        "\n",
        "- Principal Component Analysis (PCA)\n",
        "- Linear Discriminant Analysis (LDA)\n",
        "- Factor Analysis\n",
        "\n",
        "**Non-linear Methods**:\n",
        "\n",
        "- t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "- UMAP (Uniform Manifold Approximation and Projection)\n",
        "- Autoencoders\n",
        "\n",
        "In this notebook, we'll explore two popular techniques:\n",
        "\n",
        "- PCA: A linear method that finds orthogonal directions of maximum variance\n",
        "- t-SNE: A non-linear method that preserves local structure and is particularly good at revealing clusters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Principal Component Analysis (PCA)\n",
        "\n",
        "PCA is a dimensionality reduction technique that transforms high-dimensional data into a new set of orthogonal features called principal components, which capture the maximum variance in the data.\n",
        "\n",
        "Strengths of PCA:\n",
        "\n",
        "- Simple to understand and implement\n",
        "- Computationally efficient, especially for large datasets\n",
        "- Preserves global structure and variance of the data\n",
        "- Reduces noise by focusing on directions of maximum variance\n",
        "- Useful for feature extraction and data compression\n",
        "- Results are interpretable since components are linear combinations of original features\n",
        "\n",
        "Weaknesses of PCA:\n",
        "\n",
        "- Only captures linear relationships between features\n",
        "- May lose important information if data has nonlinear patterns\n",
        "- Sensitive to outliers and scaling of features\n",
        "- Principal components can be hard to interpret in terms of original features\n",
        "- Assumes orthogonality between components which may not reflect true data structure\n",
        "- Cannot handle categorical variables directly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Remove the 'insu' column from the dataset\n",
        "df_reduced = df.drop(columns=[])\n",
        "\n",
        "# Apply PCA to reduce the dataset to 3 components\n",
        "pca = PCA(n_components=3)\n",
        "principal_components = pca.fit_transform(df_reduced.iloc[:, :-1])\n",
        "\n",
        "# Create a new DataFrame with the principal components\n",
        "pca_df = pd.DataFrame(principal_components, columns=['PC1', 'PC2', 'PC3'])\n",
        "pca_df['class'] = df['class']\n",
        "\n",
        "# Add original features to the DataFrame for hover information\n",
        "for col in df_reduced.columns[:-1]:\n",
        "    pca_df[col] = df_reduced[col]\n",
        "\n",
        "# Visualize the PCA results using Plotly in 3D\n",
        "fig = px.scatter_3d(pca_df, x='PC1', y='PC2', z='PC3', color='class', title='3D PCA of Diabetes Dataset',\n",
        "                    hover_data=df_reduced.columns[:-1])\n",
        "fig.update_traces(marker=dict(size=3, line=dict(width=2, color='DarkSlateGrey')))  # Make the points smaller and add outline\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "\n",
        "t-SNE is a nonlinear dimensionality reduction technique that is particularly well-suited for visualizing high-dimensional datasets. Unlike PCA, t-SNE focuses on preserving local structure and revealing clusters in the data.\n",
        "\n",
        "Strengths of t-SNE:\n",
        "\n",
        "- Excellent at preserving local structure and revealing clusters\n",
        "- Can capture nonlinear relationships in the data\n",
        "- Particularly effective for visualization in 2D or 3D\n",
        "- Works well with high-dimensional data\n",
        "- Can reveal patterns that linear methods like PCA might miss\n",
        "- Good at separating clusters of different scales\n",
        "\n",
        "Weaknesses of t-SNE:\n",
        "\n",
        "- Computationally intensive, especially for large datasets\n",
        "- Non-deterministic - different runs can produce different results\n",
        "- Cannot meaningfully preserve global structure/distances\n",
        "- Results depend heavily on hyperparameters (perplexity, learning rate)\n",
        "- Can sometimes create artificial clusters from noise\n",
        "- Not suitable for dimensionality reduction as a preprocessing step\n",
        "- Difficult to project new data points into existing embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove the 'insu' column from the dataset before applying t-SNE\n",
        "df_reduced_tsne = df.drop(columns=[])\n",
        "\n",
        "# Apply t-SNE to reduce the dataset to 3 components with optimized parameters\n",
        "tsne = TSNE(n_components=3,  random_state=44)\n",
        "tsne_components = tsne.fit_transform(df_reduced_tsne.iloc[:, :-1])\n",
        "\n",
        "# Create a new DataFrame with the t-SNE components\n",
        "tsne_df = pd.DataFrame(tsne_components, columns=['Dim1', 'Dim2', 'Dim3'])\n",
        "tsne_df['class'] = df['class']\n",
        "\n",
        "# Add original features to the DataFrame for hover information\n",
        "for col in df_reduced_tsne.columns[:-1]:\n",
        "    tsne_df[col] = df_reduced_tsne[col]\n",
        "\n",
        "# Visualize the t-SNE results using Plotly in 3D\n",
        "fig = px.scatter_3d(tsne_df, x='Dim1', y='Dim2', z='Dim3', color='class', title='3D t-SNE Visualization of Diabetes Dataset',\n",
        "                    hover_data=df_reduced_tsne.columns[:-1])\n",
        "fig.update_traces(marker=dict(size=3, line=dict(width=2, color='DarkSlateGrey')))  # Make the points smaller and add outline\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### K-Means Clustering\n",
        "\n",
        "K-means is a popular clustering algorithm used to group data points into a predefined number of clusters.\n",
        "It works by iteratively assigning each data point to the cluster with the nearest centroid and then updating the centroids based on the new cluster assignments.\n",
        "The process continues until the centroids stabilize or a maximum number of iterations is reached.\n",
        "\n",
        "How K-Means Clustering Works:\n",
        "\n",
        "- Initialize k centroids randomly from the data points.\n",
        "- Assign each data point to the nearest centroid, forming k clusters.\n",
        "- Update the centroids by calculating the mean of all data points in each cluster.\n",
        "- Reassign each data point to the nearest centroid based on the updated centroids.\n",
        "- Repeat the update and reassignment steps until the centroids no longer change significantly or a maximum number of iterations is reached.\n",
        "- The final clusters are formed based on the last assignment of data points to centroids.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Applying K-Means\n",
        "\n",
        "We will apply k-means clustering to the Diabetes dataset and visualize the clusters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply K-means with 3 clusters\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "tsne_df['cluster'] = kmeans.fit_predict(tsne_df.iloc[:, :3])\n",
        "\n",
        "# Create two separate plots - one colored by cluster, one by class\n",
        "fig1 = px.scatter_3d(tsne_df, x='Dim1', y='Dim2', z='Dim3', color='cluster', title='3D K-Means Clustering of Diabetes Dataset',\n",
        "                    hover_data=tsne_df.columns[:-1])\n",
        "fig1.update_traces(marker=dict(size=3, line=dict(width=2, color='DarkSlateGrey')))  # Make the points smaller and add outline\n",
        "fig1.show()\n",
        "\n",
        "fig2 = px.scatter_3d(tsne_df, x='Dim1', y='Dim2', z='Dim3', color='class', title='3D K-Means Clustering of Diabetes Dataset (Colored by Class)',\n",
        "                    hover_data=tsne_df.columns[:-1])\n",
        "fig2.update_traces(marker=dict(size=3, line=dict(width=2, color='DarkSlateGrey')))  # Make the points smaller and add outline\n",
        "fig2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cluster Analysis\n",
        "\n",
        "Let's compare the clusters obtained with the actual diabetes diagnosis if available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare clusters with actual target labels if available\n",
        "if 'class' in df.columns:\n",
        "    comparison_table = pd.crosstab(tsne_df['class'], tsne_df['cluster'])\n",
        "    print(\"Comparison of clusters with actual target labels:\")\n",
        "    print(comparison_table)\n",
        "    print(\"\\nThe rows represent the actual classes, and the columns represent the clusters assigned by K-Means.\")\n",
        "    print(\"Each cell value indicates the number of instances of the actual class that were assigned to the corresponding cluster.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hierarchical Clustering\n",
        "\n",
        "Hierarchical clustering is another popular clustering technique that builds a hierarchy of clusters. It can be represented using a dendrogram, which is a tree-like diagram that records the sequences of merges or splits.\n",
        "The algorithm works by:\n",
        "\n",
        "1. Starting with each data point as its own cluster\n",
        "2. Finding the two closest clusters and merging them into a new cluster\n",
        "3. Repeating step 2 until all points are in a single cluster\n",
        "   There are two main approaches:\n",
        "\n",
        "- Agglomerative (bottom-up): Starts with individual points and merges them\n",
        "- Divisive (top-down): Starts with one cluster and splits it recursively\n",
        "  The distance between clusters can be measured in different ways:\n",
        "- Single linkage: Minimum distance between points in clusters\n",
        "- Complete linkage: Maximum distance between points in clusters\n",
        "- Average linkage: Average distance between all pairs of points\n",
        "- Ward's method: Minimizes variance within clusters\n",
        "  The resulting hierarchy can be visualized as a dendrogram tree, where:\n",
        "- Height shows the distance/dissimilarity between merged clusters\n",
        "- Vertical lines represent clusters being merged\n",
        "- Horizontal lines show the distance at which clusters merge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When to Use Hierarchical Clustering\n",
        "Hierarchical clustering is particularly useful in scenarios where:\n",
        "1. You want to discover hierarchical relationships in your data\n",
        "   - Exploring taxonomies (e.g., biological species classification)\n",
        "   - Understanding organizational structures\n",
        "   - Analyzing social networks\n",
        "2. You don't know the number of clusters beforehand\n",
        "   - The dendrogram helps visualize potential cluster counts\n",
        "   - More flexible than k-means which requires pre-specifying k\n",
        "3. You need a detailed view of data structure\n",
        "   - Shows relationships at different levels of granularity\n",
        "   - Helps identify natural groupings and subgroupings\n",
        "4. Your data has nested or hierarchical patterns\n",
        "   - Document categorization\n",
        "   - Customer segmentation with multiple levels\n",
        "   - Gene expression analysis\n",
        "5. Dataset size is small to medium\n",
        "   - Not ideal for very large datasets due to computational complexity\n",
        "   - Generally works well with datasets under 10,000 samples\n",
        "Note: Consider computational resources as hierarchical clustering has O(n²) complexity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Applying Hierarchical Clustering\n",
        "\n",
        "We will apply hierarchical clustering to the Diabetes dataset and visualize the dendrogram.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply hierarchical clustering\n",
        "linked = linkage(df.iloc[:, :-1], method='ward')\n",
        "\n",
        "# Plot the dendrogram\n",
        "plt.figure(figsize=(10, 7))\n",
        "dendrogram(linked)\n",
        "plt.title('Hierarchical Clustering Dendrogram (Ward Method)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Form flat clusters (e.g., 3 clusters)\n",
        "# This uses the fcluster function to cut the hierarchical clustering dendrogram \n",
        "# into 3 distinct clusters and assigns each data point to one of these clusters\n",
        "df['h_cluster'] = fcluster(linked, t=3, criterion='maxclust')\n",
        "\n",
        "# Compare clusters with actual class labels if available\n",
        "# If there is a 'target' column in the dataframe, create a cross-tabulation\n",
        "# to compare how well the hierarchical clusters align with the true labels\n",
        "if 'class' in df.columns:\n",
        "    print(pd.crosstab(df['class'], df['h_cluster']))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
